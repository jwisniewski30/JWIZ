{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89732270-69a9-496c-850e-c900a3344589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "from jsonschema import validate\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data():\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    src_data_path = 'routes.jsonl.gz'\n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b90c4d-e980-4790-ba9b-a56ff285e846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = read_jsonl_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d58ac04-9f0e-4b50-8fc8-cbbdc3133f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import shutil\n",
    "import uuid\n",
    "import math\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "results_dir = current_dir.joinpath('results')\n",
    "if results_dir.exists():\n",
    "    shutil.rmtree(results_dir)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def flatten_record(record):\n",
    "    flat_record = dict()\n",
    "    for key, value in record.items():\n",
    "        if key in ['airline', 'src_airport', 'dst_airport']:\n",
    "            if isinstance(value, dict):\n",
    "                for child_key, child_value in value.items():\n",
    "                    flat_key = '{}_{}'.format(key, child_key)\n",
    "                    flat_record[flat_key] = child_value\n",
    "        else:\n",
    "            flat_record[key] = value\n",
    "    \n",
    "    return flat_record\n",
    "def create_flattened_dataset():\n",
    "    records = read_jsonl_data()\n",
    "    parquet_path = results_dir.joinpath('routes-flattened.parquet')\n",
    "    return pd.DataFrame.from_records([flatten_record(record) for record in records])\n",
    "df = create_flattened_dataset()\n",
    "df['key'] = df['src_airport_iata'].astype(str) + df['dst_airport_iata'].astype(str) + df['airline_iata'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c190a49-4d27-462a-82ed-ed9aeabff87a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7.1a\n",
    "# Create the 'kv_key' column using the first letter of the 'key' column\n",
    "df['kv_key'] = df['key'].str[0].str.upper()\n",
    "\n",
    "# Map 'kv_key' to folder names based on partitions\n",
    "partitions = (\n",
    "    ('A', 'A'), ('B', 'B'), ('C', 'D'), ('E', 'F'),\n",
    "    ('G', 'H'), ('I', 'J'), ('K', 'L'), ('M', 'M'),\n",
    "    ('N', 'N'), ('O', 'P'), ('Q', 'R'), ('S', 'T'),\n",
    "    ('U', 'U'), ('V', 'V'), ('W', 'X'), ('Y', 'Z')\n",
    ")\n",
    "\n",
    "folder_mapping = {}\n",
    "for folder_range in partitions:\n",
    "    start, end = folder_range\n",
    "    folder_name = f'{start}-{end}'\n",
    "    folder_mapping[folder_range] = folder_name\n",
    "\n",
    "df['kv_key'] = df['kv_key'].apply(lambda x: next((folder_mapping[part] for part in partitions if part[0] <= x <= part[1]), None))\n",
    "\n",
    "# Save the partitioned dataset using to_parquet with partition_cols=['kv_key']\n",
    "df.to_parquet('results/kv', partition_cols=['kv_key'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed08352a-5d0c-4825-bc06-c7c2e5041f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7.1b\n",
    "# Function to create SHA256 hash of the input key and return a hexadecimal string representation of the hash\n",
    "def hash_key(key):\n",
    "    m = hashlib.sha256()\n",
    "    m.update(str(key).encode('utf-8'))\n",
    "    return m.hexdigest()\n",
    "\n",
    "# Step 4: Create the 'hashed' column using the hash function on the 'key' column\n",
    "df['hashed'] = df['key'].apply(hash_key)\n",
    "\n",
    "# Step 5: Extract the first character of the hexadecimal hash to create the partition column\n",
    "df['hash_partition'] = df['hashed'].str[0].str.upper()\n",
    "\n",
    "# Step 6: Save the partitioned dataset using to_parquet with partition_cols=['hash_partition']\n",
    "df.to_parquet('results/hash', partition_cols=['hash_partition'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8af33dd-c1f7-45dd-a9d7-66aaff42035f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7.1c\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Function to calculate the distance between two geographical coordinates\n",
    "def calculate_distance(row, data_center_coords):\n",
    "    src_coords = (row['src_latitude'], row['src_longitude'])\n",
    "    distances = [geodesic(src_coords, data_center_coords[dc]).miles for dc in data_center_coords]\n",
    "    return list(data_center_coords.keys())[distances.index(min(distances))]\n",
    "\n",
    "# Data center coordinates\n",
    "data_center_coords = {\n",
    "    'central': (41.1544433, -96.0422378),    # Papillion, NE\n",
    "    'east': (39.08344, -77.6497145),        # Loudoun County, Virginia\n",
    "    'west': (45.5945645, -121.1786823)      # The Dalles, Oregon\n",
    "}\n",
    "\n",
    "# Add geographical coordinates for source airports\n",
    "df['src_latitude'] = df['src_airport_latitude']\n",
    "df['src_longitude'] = df['src_airport_longitude']\n",
    "\n",
    "# Drop rows with missing latitude and longitude values\n",
    "df.dropna(subset=['src_latitude', 'src_longitude'], inplace=True)\n",
    "\n",
    "# Calculate the closest data center to each source airport and create the 'location' column\n",
    "df['location'] = df.apply(lambda row: calculate_distance(row, data_center_coords), axis=1)\n",
    "\n",
    "# Save the partitioned dataset using to_parquet with partition_cols=['location']\n",
    "df.to_parquet('results/geo', partition_cols=['location'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68a450a9-2902-4cf1-ad8b-9190c08d132f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#7.1d\n",
    "def balance_partitions(keys, num_partitions):\n",
    "    # Sort the keys\n",
    "    sorted_keys = sorted(keys)\n",
    "\n",
    "    # Calculate the number of keys in each partition\n",
    "    keys_per_partition = len(keys) // num_partitions\n",
    "    remainder = len(keys) % num_partitions\n",
    "\n",
    "    # Initialize the starting index of each partition\n",
    "    partition_indices = [0]\n",
    "    for i in range(1, num_partitions):\n",
    "        partition_indices.append(partition_indices[-1] + keys_per_partition + (1 if i <= remainder else 0))\n",
    "\n",
    "    # Create partitions\n",
    "    partitions = [sorted_keys[partition_indices[i]:partition_indices[i + 1]] for i in range(num_partitions - 1)]\n",
    "    # Add the last partition separately to handle the remaining keys\n",
    "    partitions.append(sorted_keys[partition_indices[-1]:])\n",
    "\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f36db4a7-bb29-48a2-9b8b-578a44e61493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 10, 15, 20], [25, 30, 35], [40, 45, 50]]\n"
     ]
    }
   ],
   "source": [
    "keys = [10, 20, 5, 15, 25, 30, 35, 40, 45, 50]\n",
    "num_partitions = 3\n",
    "\n",
    "partitions = balance_partitions(keys, num_partitions)\n",
    "print(partitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
